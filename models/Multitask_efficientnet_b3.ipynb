{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46537662",
   "metadata": {},
   "source": [
    "### Classification notebook for Kaggle RSNA competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d241113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:40.770422Z",
     "iopub.status.busy": "2022-10-21T04:46:40.768650Z",
     "iopub.status.idle": "2022-10-21T04:46:46.027848Z",
     "shell.execute_reply": "2022-10-21T04:46:46.026647Z"
    },
    "id": "_b9cHesklRKW",
    "outputId": "0251b11b-0475-4dbb-dd52-7b75ec6d27e9",
    "papermill": {
     "duration": 5.275668,
     "end_time": "2022-10-21T04:46:46.030655",
     "exception": false,
     "start_time": "2022-10-21T04:46:40.754987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold,StratifiedGroupKFold\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from requests import get\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import timm\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2,torchvision\n",
    "from ipyexperiments.ipyexperiments import IPyExperimentsPytorch\n",
    "from timm.optim.optim_factory import create_optimizer_v2\n",
    "from timm import utils\n",
    "from fastprogress.fastprogress import format_time\n",
    "from fastai.vision.all import *\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class CFG:\n",
    "    seed = 46\n",
    "    n_splits = 4\n",
    "    SZ = (1536, 960)\n",
    "    debug = False\n",
    "    BS = 10\n",
    "    EP = 12\n",
    "    MODEL = 'efficientnet_b3'\n",
    "    LR = 4e-04\n",
    "    WD = 1e-08\n",
    "\n",
    "random.seed(CFG.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe7694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:46.053836Z",
     "iopub.status.busy": "2022-10-21T04:46:46.053171Z",
     "iopub.status.idle": "2022-10-21T04:46:46.061921Z",
     "shell.execute_reply": "2022-10-21T04:46:46.060953Z"
    },
    "papermill": {
     "duration": 0.022786,
     "end_time": "2022-10-21T04:46:46.064147",
     "exception": false,
     "start_time": "2022-10-21T04:46:46.041361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a4371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:46.086954Z",
     "iopub.status.busy": "2022-10-21T04:46:46.086674Z",
     "iopub.status.idle": "2022-10-21T04:46:46.149851Z",
     "shell.execute_reply": "2022-10-21T04:46:46.148778Z"
    },
    "id": "PD4IsNvglQYA",
    "papermill": {
     "duration": 0.077084,
     "end_time": "2022-10-21T04:46:46.152797",
     "exception": false,
     "start_time": "2022-10-21T04:46:46.075713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = '///mnt/c/Personal/Competitions/Kaggle/rsna'\n",
    "image_dir = f'{root_dir}/data/8bit'\n",
    "DIR = '///mnt/c/Personal/Competitions/Kaggle/rsna/data/'\n",
    "submit = pd.read_csv(os.path.join(DIR,'sample_submission.csv'))\n",
    "train = pd.read_csv(os.path.join(DIR,'Train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DIR,'Test.csv'))\n",
    "\n",
    "if CFG.debug:\n",
    "    train = train.sample(frac=0.01).reset_index(drop=True)\n",
    "    \n",
    "VERSION = \"Multitask_EB3\"\n",
    "MODEL_FOLDER = Path(f\"{root_dir}/runs/{VERSION}/\")\n",
    "os.makedirs(MODEL_FOLDER,exist_ok=True)\n",
    "KERNEL_TYPE = f\"{CFG.MODEL}_{CFG.SZ[0]}_{CFG.SZ[1]}_bs{CFG.BS}_ep{CFG.EP}_lr{str(CFG.LR).replace('-','')}_wd{str(CFG.WD).replace('-','')}\"\n",
    "\n",
    "print(MODEL_FOLDER)\n",
    "print(KERNEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed555320",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['difficult_negative_case'] = train['difficult_negative_case'].astype(int)\n",
    "train['laterality_enc'] = train['laterality'].map(dict({'L':0,'R':1}))\n",
    "train['view_enc'] = train['view'].map(dict({'CC':0,'MLO':1,'ML':2,'LM':3,'AT':4,'LMO':5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64491567",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_labels = ['laterality_enc','implant','view_enc'] #'biopsy','invasive'\n",
    "train[aux_labels].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[aux_labels].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd909d43",
   "metadata": {
    "papermill": {
     "duration": 0.010524,
     "end_time": "2022-10-21T04:46:47.308030",
     "exception": false,
     "start_time": "2022-10-21T04:46:47.297506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5e8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:50.415560Z",
     "iopub.status.busy": "2022-10-21T04:46:50.415286Z",
     "iopub.status.idle": "2022-10-21T04:46:50.457720Z",
     "shell.execute_reply": "2022-10-21T04:46:50.456777Z"
    },
    "papermill": {
     "duration": 0.057434,
     "end_time": "2022-10-21T04:46:50.459934",
     "exception": false,
     "start_time": "2022-10-21T04:46:50.402500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mskf = StratifiedGroupKFold(n_splits=CFG.n_splits, shuffle=True, random_state=121)\n",
    "fold_ids = []\n",
    "train['fold'] = 0\n",
    "\n",
    "for train_index, test_index in mskf.split(train,train['cancer'].values,train['patient_id'].values):\n",
    "    fold_ids.append(test_index)    \n",
    "\n",
    "for fld in range(CFG.n_splits):\n",
    "    valIx = fold_ids[fld]\n",
    "    train.loc[valIx,'fold']=fld "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb2b79",
   "metadata": {
    "papermill": {
     "duration": 0.011456,
     "end_time": "2022-10-21T04:46:50.483973",
     "exception": false,
     "start_time": "2022-10-21T04:46:50.472517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36482fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(d):\n",
    "    image = cv2.imread(os.path.join(image_dir,f'{d.patient_id}_{d.image_id}.png'))\n",
    "    return image\n",
    "\n",
    "class RsnaDataset(Dataset):\n",
    "    def __init__(self, df, augs=None,mode='train'):\n",
    "        self.length = len(df)\n",
    "        self.df = df\n",
    "        self.augs = augs\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        image = read_data(d)\n",
    "        image = image.astype(np.float32)/255\n",
    "        if self.augs is not None:\n",
    "            image = self.augs(image=image)['image']\n",
    "        patient_id = d.patient_id\n",
    "        \n",
    "        aux1 = torch.tensor(d.laterality_enc).float()\n",
    "        aux2 = torch.tensor(d.biopsy).float()        \n",
    "#         aux3 = torch.tensor(d.invasive).float()\n",
    "        aux3 = torch.tensor(d.view_enc).long()\n",
    "        aux4 = torch.tensor(d.implant).float()\n",
    "        cancer = torch.tensor(d.cancer).float()\n",
    "        \n",
    "        if self.mode=='test':\n",
    "            return image,patient_id\n",
    "        \n",
    "        return image,cancer,aux1,aux2,aux3,aux4,patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101207aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:51.351557Z",
     "iopub.status.busy": "2022-10-21T04:46:51.351216Z",
     "iopub.status.idle": "2022-10-21T04:46:51.356377Z",
     "shell.execute_reply": "2022-10-21T04:46:51.355263Z"
    },
    "papermill": {
     "duration": 0.025285,
     "end_time": "2022-10-21T04:46:51.358947",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.333662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Handles PyTorch x Numpy seeding issues.\n",
    "    Args:\n",
    "        worker_id (int): Id of the worker.\n",
    "    \"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b59ef",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ddfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUG = A.Compose([\n",
    "    A.ShiftScaleRotate(rotate_limit=15, p=0.7),\n",
    "    A.HorizontalFlip(p = 0.5),\n",
    "    A.Resize(CFG.SZ[0],CFG.SZ[1]),\n",
    "    # A.Normalize(mean=0,std=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "VALID_AUG = A.Compose([\n",
    "    A.Resize(CFG.SZ[0],CFG.SZ[1]),\n",
    "    # A.Normalize(mean=0,std=1),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf59ae",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_show = RsnaDataset(train, augs=TRAIN_AUG, mode='train')\n",
    "loader_show = torch.utils.data.DataLoader(dataset_show, batch_size=8)\n",
    "img,target,aux_targ1,aux_targ2,aux_targ3,aux_targ4,_ = next(iter(loader_show))\n",
    "\n",
    "grid = torchvision.utils.make_grid(img, normalize=True, padding=2)\n",
    "grid = grid.permute(1, 2, 0)\n",
    "show_image(grid, figsize=(15,8),title=[x for x in target.numpy()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f9823",
   "metadata": {
    "papermill": {
     "duration": 0.012058,
     "end_time": "2022-10-21T04:46:51.383502",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.371444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20034eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsna_classification_model(model_name, pretrained=True, **kwargs):\n",
    "    model = timm.create_model(CFG.MODEL, pretrained=pretrained,num_classes=0, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.efficientnet import *\n",
    "class Net(nn.Module):\n",
    "    def load_pretrain(self, ):\n",
    "        pass\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.encoder = tf_efficientnetv2_s(pretrained=True)#,drop_rate = 0.15, drop_path_rate = 0.15)\n",
    "        #encoder_dim = [64, 256, 512, 1024, 2048]\n",
    "        try:\n",
    "            k = self.encoder.classifier.in_features\n",
    "        except:\n",
    "            k = self.encoder.classifier[1].in_features\n",
    "            \n",
    "        self.cancer = nn.Linear(k,1)\n",
    "        self.aux1 = nn.Linear(k,1)\n",
    "        self.aux2 = nn.Linear(k,1)\n",
    "        self.aux3 = nn.Linear(k,6)\n",
    "        self.aux4 = nn.Linear(k,1)\n",
    "    def forward(self, x):\n",
    "        #------\n",
    "        e = self.encoder\n",
    "        x = e.forward_features(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = torch.flatten(x,1,3)\n",
    "        #------\n",
    "\n",
    "        feature = x\n",
    "        cancer = self.cancer(feature).reshape(-1)\n",
    "        aux1 = self.aux1(feature).reshape(-1)\n",
    "        aux2 = self.aux2(feature).reshape(-1)\n",
    "        aux3 = self.aux3(feature)#.reshape(-1)\n",
    "        aux4 = self.aux4(feature).reshape(-1)\n",
    "        return cancer, aux1,aux2,aux3,aux4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41add6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:51.748231Z",
     "iopub.status.busy": "2022-10-21T04:46:51.747517Z",
     "iopub.status.idle": "2022-10-21T04:46:51.959456Z",
     "shell.execute_reply": "2022-10-21T04:46:51.958430Z"
    },
    "papermill": {
     "duration": 0.227045,
     "end_time": "2022-10-21T04:46:51.962222",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.735177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(RsnaDataset(train, augs=TRAIN_AUG, mode='train'),\n",
    "                          batch_size=2,\n",
    "                          shuffle=True,\n",
    "                          num_workers=8,\n",
    "                          drop_last=True,\n",
    "                        worker_init_fn=worker_init_fn)\n",
    "\n",
    "image,cancer,aux1,aux2,aux31,aux4,patient_id = next(iter(dl))\n",
    "# a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3e892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:51.987500Z",
     "iopub.status.busy": "2022-10-21T04:46:51.987165Z",
     "iopub.status.idle": "2022-10-21T04:46:58.618975Z",
     "shell.execute_reply": "2022-10-21T04:46:58.616805Z"
    },
    "papermill": {
     "duration": 6.647161,
     "end_time": "2022-10-21T04:46:58.621465",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.974304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m = get_rsna_classification_model(CFG.MODEL)\n",
    "m = Net()\n",
    "cancer1, aux1,aux2,aux3,aux4 = m(image)\n",
    "print(cancer, aux1.shape,aux2.shape,aux3.shape,aux4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fb85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.BCEWithLogitsLoss()(cancer,cancer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss()(aux3,aux31)  ## nn.CrossEntropyLoss()(pred,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bc3e1",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalanceSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset, ratio=8):\n",
    "        self.r = ratio-1\n",
    "        self.dataset = dataset\n",
    "        self.pos_index = np.where(dataset.df.cancer>0)[0]\n",
    "        self.neg_index = np.where(dataset.df.cancer==0)[0]\n",
    "\n",
    "        self.length = self.r*int(np.floor(len(self.neg_index)/self.r))\n",
    "\n",
    "    def __iter__(self):\n",
    "        pos_index = self.pos_index.copy()\n",
    "        neg_index = self.neg_index.copy()\n",
    "        np.random.shuffle(pos_index)\n",
    "        np.random.shuffle(neg_index)\n",
    "\n",
    "        neg_index = neg_index[:self.length].reshape(-1,self.r)\n",
    "        pos_index = np.random.choice(pos_index, self.length//self.r).reshape(-1,1)\n",
    "\n",
    "        index = np.concatenate([pos_index,neg_index],-1).reshape(-1)\n",
    "        return iter(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eabf0c",
   "metadata": {},
   "source": [
    "### Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185562dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Customn losss fnc\n",
    "\n",
    "### Customn losss fnc\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = F.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        return torch.mean(focal_loss)\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module): \n",
    "    def __init__(self, classes=1, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "    \n",
    "class CustomAuxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CustomAuxLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "\n",
    "    def forward(self, logits, target, auxLogits1, auxtarget1, auxLogits2, auxtarget2,\n",
    "               auxLogits3, auxtarget3,auxLogits4, auxtarget4):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        \n",
    "        BCELoss = F.binary_cross_entropy_with_logits(logits,target)\n",
    "        BCELoss_aux1 = F.binary_cross_entropy_with_logits(auxLogits1,auxtarget1)\n",
    "        BCELoss_aux2 = F.binary_cross_entropy_with_logits(auxLogits2,auxtarget2)\n",
    "        BCELoss_aux3 = nn.CrossEntropyLoss()(auxLogits3,auxtarget3)\n",
    "        BCELoss_aux4 = F.binary_cross_entropy_with_logits(auxLogits4,auxtarget4)\n",
    "\n",
    "        BCELoss_aux = (BCELoss_aux1+BCELoss_aux2+BCELoss_aux3+BCELoss_aux4)/4   \n",
    "        loss = BCELoss*self.alpha+BCELoss_aux*(1-self.alpha)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff2ad",
   "metadata": {
    "papermill": {
     "duration": 0.013027,
     "end_time": "2022-10-21T04:46:58.648053",
     "exception": false,
     "start_time": "2022-10-21T04:46:58.635026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train & Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597af09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:58.675824Z",
     "iopub.status.busy": "2022-10-21T04:46:58.674778Z",
     "iopub.status.idle": "2022-10-21T04:46:58.683272Z",
     "shell.execute_reply": "2022-10-21T04:46:58.682071Z"
    },
    "papermill": {
     "duration": 0.025267,
     "end_time": "2022-10-21T04:46:58.685455",
     "exception": false,
     "start_time": "2022-10-21T04:46:58.660188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pfbeta(labels, preds, beta=1,clip=True):\n",
    "    if clip:\n",
    "        preds = preds.clip(0, 1)\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "def pfbeta_thresh(labels, preds, beta=1):\n",
    "    preds = preds>0.1\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "def optimal_f1(labels, predictions):\n",
    "    labels = labels.cpu().numpy()\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    thres = np.linspace(0, 1, 100)\n",
    "    f1s = [pfbeta(labels, predictions > thr,clip=False) for thr in thres]\n",
    "    idx = np.argmax(f1s)\n",
    "    return f1s[idx], thres[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c498a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: Iterable,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler._LRScheduler = None,\n",
    "    mixup_fn: Callable = None,\n",
    "    grad_scaler: torch.cuda.amp.GradScaler = None,\n",
    "    mbar: master_bar = None,\n",
    "):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses_m = utils.AverageMeter()\n",
    "\n",
    "    pbar = progress_bar(loader, parent=mbar, leave=False)\n",
    "    pbar.update(0)\n",
    "\n",
    "    for batch_idx, (input, target,aux_target1,aux_target2,aux_target3,aux_target4,_) in enumerate(loader):\n",
    "        input, target, aux_target1, aux_target2,aux_target3,aux_target4 = input.cuda(), target.cuda(), aux_target1.cuda(),aux_target2.cuda(),aux_target3.cuda(),aux_target4.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output,aux_output1,aux_output2,aux_output3,aux_output4 = model(input)\n",
    "            loss = loss_fn(output, target,aux_output1,aux_target1,aux_output2,aux_target2,aux_output3,aux_target3,aux_output4,aux_target4)\n",
    "        losses_m.update(loss.item(), input.size(0))\n",
    "\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        pbar.update(batch_idx + 1)\n",
    "        pbar.comment = f\"{losses_m.avg:.4f}\"\n",
    "\n",
    "    pbar.on_iter_end()\n",
    "    return OrderedDict([(\"loss\", losses_m.avg)])\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def validate(model: nn.Module, loader: Iterable, loss_fn: Callable, mbar: master_bar):\n",
    "    model.eval()\n",
    "\n",
    "    metric_m = utils.AverageMeter()\n",
    "    metric_m_thresh = utils.AverageMeter()\n",
    "    auc_m = utils.AverageMeter()\n",
    "    losses_m = utils.AverageMeter()\n",
    "\n",
    "    pbar = progress_bar(loader, parent=mbar, leave=False)\n",
    "    pbar.update(0)\n",
    "\n",
    "    for batch_idx, (input, target,_,_,_,_,_) in enumerate(loader):\n",
    "        \n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        output,_,_,_,_ = model(input)\n",
    "        loss = loss_fn(output, target).item()\n",
    "        losses_m.update(loss, input.size(0))\n",
    "        \n",
    "        output = F.sigmoid(output)\n",
    "        metric = pfbeta(target,output).item()\n",
    "        metric_thresh,_ = optimal_f1(target, output)\n",
    "        # pfbeta_thresh(target,output).item()\n",
    "        metric_m.update(metric, output.size(0))\n",
    "        metric_m_thresh.update(metric_thresh.item(), output.size(0))\n",
    "        pbar.update(batch_idx + 1)\n",
    "\n",
    "    pbar.on_iter_end()\n",
    "    return OrderedDict([(\"loss\", losses_m.avg), (\"metric\", metric_m.avg),(\"metric_thresh\", metric_m_thresh.avg)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee4a01",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60917654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(fold):\n",
    "    \n",
    "    with IPyExperimentsPytorch(exp_enable=False, cl_set_seed=42, cl_compact=True):\n",
    "        print()\n",
    "        print(\"*\" * 100)\n",
    "        print(f\"Training fold {fold}\")\n",
    "        print(\"*\" * 100)\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "      \n",
    "        dataset_train = RsnaDataset(train.query(\"fold!=@fold\").reset_index(drop=True), augs=TRAIN_AUG, mode=\"train\")\n",
    "        dataset_valid = RsnaDataset(train.query(\"fold==@fold\").reset_index(drop=True), augs=VALID_AUG, mode=\"valid\")\n",
    "\n",
    "        print(f\"TRAIN: {len(dataset_train)} | VALID: {len(dataset_valid)}\")\n",
    "\n",
    "        loader_train = torch.utils.data.DataLoader(dataset_train, \n",
    "                                                   CFG.BS, \n",
    "                                                   num_workers=8, \n",
    "                                                   drop_last=True,\n",
    "                                                  pin_memory=True)#,\n",
    "#                                                    sampler=ImbalancedDatasetSampler(dataset_train))\n",
    "        loader_valid = torch.utils.data.DataLoader(dataset_valid, CFG.BS * 2, num_workers=8, shuffle=False)\n",
    "\n",
    "        model = Net()\n",
    "        model.cuda()\n",
    "        optimizer = create_optimizer_v2(model, \"lookahead_RAdam\", lr=CFG.LR)\n",
    "\n",
    "        num_train_steps = len(loader_train) * CFG.EP\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=CFG.LR, total_steps=num_train_steps,verbose=False)\n",
    "\n",
    "        train_loss_fn = CustomAuxLoss(alpha=0.8) #nn.BCEWithLogitsLoss()\n",
    "        valid_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        grad_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        print(f\"Scheduled epochs: {CFG.EP}\")\n",
    "\n",
    "        mbar = master_bar(list(range(CFG.EP)))\n",
    "        best_epoch, best_metric = 0, 100\n",
    "        metric_names = [\"epoch\", \"train_loss\", \"valid_loss\", \"metric\",\"metric_thresh\", \"time\"]\n",
    "        mbar.write([f\"{l:.6f}\" if isinstance(l, float) else str(l) for l in metric_names], table=True)\n",
    "        for epoch in range(CFG.EP):\n",
    "            \n",
    "                       \n",
    "            start_time = time.time()\n",
    "            mbar.update(epoch)\n",
    "            \n",
    "            train_metrics = train_one_epoch(\n",
    "                model, loader_train, train_loss_fn, optimizer,\n",
    "                lr_scheduler=lr_scheduler, mixup_fn=None, grad_scaler=grad_scaler, mbar=mbar)\n",
    "\n",
    "            valid_metrics = validate(model, loader_valid, valid_loss_fn, mbar=mbar)\n",
    "            \n",
    "            elapsed = format_time(time.time() - start_time)\n",
    "            epoch_log = [epoch,train_metrics[\"loss\"], valid_metrics[\"loss\"], valid_metrics[\"metric\"],\n",
    "                         valid_metrics[\"metric_thresh\"], elapsed]\n",
    "            mbar.write([f\"{l:.6f}\" if isinstance(l, float) else str(l) for l in epoch_log], table=True)\n",
    "\n",
    "            if 1:\n",
    "                best_epoch, best_metric = epoch, valid_metrics[\"loss\"]\n",
    "                path = Path(f'{MODEL_FOLDER}/fold_{fold}')\n",
    "                os.makedirs(path,exist_ok=True)\n",
    "                dirpath = path / (KERNEL_TYPE + f\"_Epoch_{epoch}_fold_{fold}.pth\")\n",
    "                torch.save(model.state_dict(), dirpath)\n",
    "            \n",
    "        mbar.on_iter_end()\n",
    "        print(\"*** Best metric: {0} (epoch {1})\".format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7877d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for fold_idx in [0,1,2,3]:\n",
    "        training_loop(fold_idx)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6788d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_oof(fold):\n",
    "   \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dataset_valid = RsnaDataset(train.query(\"fold==@fold\").reset_index(drop=True), augs=VALID_AUG, mode=\"valid\")\n",
    "    ix =  train.query(\"fold==@fold\").index\n",
    "    print(f\"VALID: {len(dataset_valid)}\")\n",
    "\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, CFG.BS * 2, num_workers=8, shuffle=False)\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(f'{MODEL_FOLDER}/fold_{fold}/{KERNEL_TYPE}_Epoch_{CFG.EP-1}_fold_{fold}.pth'))\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    preds = []\n",
    "    imageids = []\n",
    "\n",
    "    for input,label,_,_,_,_,patient_id in tqdm(loader_valid, dynamic_ncols=True, desc=\"OOF Generation\"):\n",
    "        pred = []\n",
    "        with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "            input = input.cuda()\n",
    "            pred.append(F.sigmoid(model(input)[0]))\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        preds.append(torch.concat(pred).data.cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0),ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7efbb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof = np.zeros((len(train)))\n",
    "for k in tqdm([0,1,2,3]):\n",
    "    oof_fold,ix = gen_oof(k)\n",
    "    print(oof_fold.min(),oof_fold.max())\n",
    "    oof[ix] += oof_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c476ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_f1_numpy(oof,fold):\n",
    "    labels = train.loc[train['fold'].isin(fold)].reset_index(drop=True)['cancer'].values\n",
    "    oof = oof[train.loc[train['fold'].isin(fold)].index]\n",
    "    thres = np.linspace(0, 1, 100)\n",
    "    f1s = [pfbeta(labels, oof > thr,clip=False) for thr in thres]\n",
    "    idx = np.argmax(f1s)\n",
    "    return f1s[idx], thres[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr, thresh = optimal_f1_numpy(oof,[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr,thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb57bf2",
   "metadata": {
    "id": "T0Z5tB5f8obu",
    "papermill": {
     "duration": 0.018419,
     "end_time": "2022-10-21T04:58:56.268413",
     "exception": false,
     "start_time": "2022-10-21T04:58:56.249994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fin "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 771.032802,
   "end_time": "2022-10-21T04:58:58.627815",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-21T04:46:07.595013",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
