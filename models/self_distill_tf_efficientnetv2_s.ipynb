{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46537662",
   "metadata": {},
   "source": [
    "### Classification notebook for Kaggle RSNA competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d241113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:40.770422Z",
     "iopub.status.busy": "2022-10-21T04:46:40.768650Z",
     "iopub.status.idle": "2022-10-21T04:46:46.027848Z",
     "shell.execute_reply": "2022-10-21T04:46:46.026647Z"
    },
    "id": "_b9cHesklRKW",
    "outputId": "0251b11b-0475-4dbb-dd52-7b75ec6d27e9",
    "papermill": {
     "duration": 5.275668,
     "end_time": "2022-10-21T04:46:46.030655",
     "exception": false,
     "start_time": "2022-10-21T04:46:40.754987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold,StratifiedGroupKFold\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from requests import get\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import timm\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2,torchvision\n",
    "from ipyexperiments.ipyexperiments import IPyExperimentsPytorch\n",
    "from timm.optim.optim_factory import create_optimizer_v2\n",
    "from timm import utils\n",
    "from fastprogress.fastprogress import format_time\n",
    "from fastai.vision.all import *\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class CFG:\n",
    "    seed = 46\n",
    "    n_splits = 4\n",
    "    SZ = (1536, 960)\n",
    "    debug = False\n",
    "    BS = 8\n",
    "    EP = 12\n",
    "    MODEL = 'tf_efficientnetv2_s'\n",
    "    LR = 1e-04\n",
    "    WD = 1e-06\n",
    "    max_norm = 10\n",
    "random.seed(CFG.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe7694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:46.053836Z",
     "iopub.status.busy": "2022-10-21T04:46:46.053171Z",
     "iopub.status.idle": "2022-10-21T04:46:46.061921Z",
     "shell.execute_reply": "2022-10-21T04:46:46.060953Z"
    },
    "papermill": {
     "duration": 0.022786,
     "end_time": "2022-10-21T04:46:46.064147",
     "exception": false,
     "start_time": "2022-10-21T04:46:46.041361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a4371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:46.086954Z",
     "iopub.status.busy": "2022-10-21T04:46:46.086674Z",
     "iopub.status.idle": "2022-10-21T04:46:46.149851Z",
     "shell.execute_reply": "2022-10-21T04:46:46.148778Z"
    },
    "id": "PD4IsNvglQYA",
    "papermill": {
     "duration": 0.077084,
     "end_time": "2022-10-21T04:46:46.152797",
     "exception": false,
     "start_time": "2022-10-21T04:46:46.075713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = '///mnt/c/Personal/Competitions/Kaggle/rsna'\n",
    "image_dir = f'{root_dir}/data/8bit'\n",
    "\n",
    "DIR = '///mnt/c/Personal/Competitions/Kaggle/rsna/data/'\n",
    "submit = pd.read_csv(os.path.join(DIR,'sample_submission.csv'))\n",
    "train = pd.read_csv(os.path.join(DIR,'Train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DIR,'Test.csv'))\n",
    "\n",
    "if CFG.debug:\n",
    "    train = train.sample(frac=0.01).reset_index(drop=True)\n",
    "\n",
    "VERSION = \"self_distill_tf_efficientnetv2_s\"\n",
    "MODEL_FOLDER = Path(f\"{root_dir}/runs/{VERSION}/\")\n",
    "os.makedirs(MODEL_FOLDER,exist_ok=True)\n",
    "KERNEL_TYPE = f\"{CFG.MODEL}_{CFG.SZ[0]}_{CFG.SZ[1]}_bs{CFG.BS}_ep{CFG.EP}_lr{str(CFG.LR).replace('-','')}_wd{str(CFG.WD).replace('-','')}\"\n",
    "\n",
    "print(MODEL_FOLDER)\n",
    "print(KERNEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed555320",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['difficult_negative_case'] = train['difficult_negative_case'].astype(int)\n",
    "train['laterality_enc'] = train['laterality'].map(dict({'L':0,'R':1}))\n",
    "train['view_enc'] = train['view'].map(dict({'CC':0,'MLO':1,'ML':2,'LM':3,'AT':4,'LMO':5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d59027",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['BIRADS'] = train['BIRADS'].fillna(3).astype(int)\n",
    "train['density'] = train['density'].fillna(\"E\").map({'A':0,'B':1,'C':2,'D':3,'E':4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd909d43",
   "metadata": {
    "papermill": {
     "duration": 0.010524,
     "end_time": "2022-10-21T04:46:47.308030",
     "exception": false,
     "start_time": "2022-10-21T04:46:47.297506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5e8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:50.415560Z",
     "iopub.status.busy": "2022-10-21T04:46:50.415286Z",
     "iopub.status.idle": "2022-10-21T04:46:50.457720Z",
     "shell.execute_reply": "2022-10-21T04:46:50.456777Z"
    },
    "papermill": {
     "duration": 0.057434,
     "end_time": "2022-10-21T04:46:50.459934",
     "exception": false,
     "start_time": "2022-10-21T04:46:50.402500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mskf = StratifiedGroupKFold(n_splits=CFG.n_splits, shuffle=True, random_state=1)\n",
    "fold_ids = []\n",
    "train['fold'] = 0\n",
    "\n",
    "for train_index, test_index in mskf.split(train,train['cancer'].values,train['patient_id'].values):\n",
    "    fold_ids.append(test_index)    \n",
    "\n",
    "for fld in range(CFG.n_splits):\n",
    "    valIx = fold_ids[fld]\n",
    "    train.loc[valIx,'fold']=fld "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb2b79",
   "metadata": {
    "papermill": {
     "duration": 0.011456,
     "end_time": "2022-10-21T04:46:50.483973",
     "exception": false,
     "start_time": "2022-10-21T04:46:50.472517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9afc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_blackarea(d):\n",
    "    X = cv2.imread(os.path.join(image_dir,f'{d.patient_id}_{d.image_id}.png'))\n",
    "    X = X[5:-5, 5:-5]\n",
    "    \n",
    "    # regions of non-empty pixels\n",
    "    output= cv2.connectedComponentsWithStats((X > 20).astype(np.uint8)[:, :, 0], 8, cv2.CV_32S)\n",
    "\n",
    "    # stats.shape == (N, 5), where N is the number of regions, 5 dimensions correspond to:\n",
    "    # left, top, width, height, area_size\n",
    "    stats = output[2]\n",
    "\n",
    "    # finding max area which always corresponds to the breast data. \n",
    "    idx = stats[1:, 4].argmax() + 1\n",
    "    x1, y1, w, h = stats[idx][:4]\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    \n",
    "    # cutting out the breast data\n",
    "    X_fit = X[y1: y2, x1: x2]\n",
    "    \n",
    "    return X_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36482fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(d):\n",
    "    image = cv2.imread(os.path.join(image_dir,f'{d.patient_id}_{d.image_id}.png'))\n",
    "    return image\n",
    "\n",
    "class RsnaDataset(Dataset):\n",
    "    def __init__(self, df, augs=None,mode='train',crop=True):\n",
    "        self.length = len(df)\n",
    "        self.df = df\n",
    "        self.augs = augs\n",
    "        self.mode = mode\n",
    "        self.crop=crop\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        \n",
    "        if self.crop:\n",
    "            image = crop_blackarea(d)\n",
    "        else:\n",
    "            image = read_data(d)\n",
    "            \n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if self.augs is not None:\n",
    "            image = self.augs(image=image)['image']\n",
    "                \n",
    "        patient_id = d.patient_id\n",
    "        \n",
    "        cancer = torch.tensor(d.cancer).float()\n",
    "        \n",
    "        if self.mode=='test':\n",
    "            return image,patient_id\n",
    "        \n",
    "        return image,cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101207aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:51.351557Z",
     "iopub.status.busy": "2022-10-21T04:46:51.351216Z",
     "iopub.status.idle": "2022-10-21T04:46:51.356377Z",
     "shell.execute_reply": "2022-10-21T04:46:51.355263Z"
    },
    "papermill": {
     "duration": 0.025285,
     "end_time": "2022-10-21T04:46:51.358947",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.333662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Handles PyTorch x Numpy seeding issues.\n",
    "    Args:\n",
    "        worker_id (int): Id of the worker.\n",
    "    \"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b59ef",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ddfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUG = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.03, rotate_limit=15, p=0.5, border_mode=0),\n",
    "    A.OneOf([A.HorizontalFlip(p = 0.5),\n",
    "    A.VerticalFlip(p = 0.5)],p=0.5),    \n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.OneOf(\n",
    "        transforms=[\n",
    "           A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.PiecewiseAffine(p=0.3)],p=0.2),\n",
    "    A.Affine(translate_percent=0.0, rotate=0, shear=0, scale=[0.8,1.2], p= 0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height= 5, max_width= 5, p=0.5),\n",
    "    A.Cutout(num_holes = 8,max_h_size = 5, max_w_size = 5, p=0.7),\n",
    "    A.Resize(CFG.SZ[0],CFG.SZ[1]),\n",
    "    A.Normalize(mean=0,std=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "VALID_AUG = A.Compose([\n",
    "    A.Resize(CFG.SZ[0],CFG.SZ[1]),\n",
    "    A.Normalize(mean=0,std=1),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf59ae",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410391e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_show = RsnaDataset(train, augs=TRAIN_AUG, mode='train')\n",
    "loader_show = torch.utils.data.DataLoader(dataset_show, batch_size=8,shuffle=False)\n",
    "img,target  = next(iter(loader_show))\n",
    "\n",
    "grid = torchvision.utils.make_grid(img, normalize=True, padding=2)\n",
    "grid = grid.permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_show = RsnaDataset(train, augs=TRAIN_AUG, mode='train',crop=False)\n",
    "loader_show = torch.utils.data.DataLoader(dataset_show, batch_size=8,shuffle=False)\n",
    "img,target  = next(iter(loader_show))\n",
    "\n",
    "grid = torchvision.utils.make_grid(img, normalize=True, padding=2)\n",
    "grid = grid.permute(1, 2, 0)\n",
    "show_image(grid, figsize=(15,8),title=[x for x in target.numpy()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f9823",
   "metadata": {
    "papermill": {
     "duration": 0.012058,
     "end_time": "2022-10-21T04:46:51.383502",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.371444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat((F.adaptive_avg_pool1d(x, 1), F.adaptive_max_pool1d(x, 1)), dim=1)\n",
    "\n",
    "    def feat_mult(self):\n",
    "        return 2\n",
    "\n",
    "\n",
    "class SwinPooler(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat((F.adaptive_avg_pool2d(x, 1), F.adaptive_max_pool2d(x, 1)), dim=1)\n",
    "\n",
    "    def feat_mult(self):\n",
    "        return 2\n",
    "\n",
    "\n",
    "def gem_1d(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool1d(x.clamp(min=eps).pow(p), (x.size(-1),)).pow(1.0 / p)\n",
    "\n",
    "\n",
    "def gem_2d(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n",
    "\n",
    "\n",
    "class GeM1d(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n",
    "        super(GeM1d, self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def feat_mult(self):\n",
    "        return 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_1d(x, p=self.p, eps=self.eps)\n",
    "\n",
    "\n",
    "class GeM2d(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n",
    "        super(GeM2d, self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def feat_mult(self):\n",
    "        return 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_2d(x, p=self.p, eps=self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = timm.create_model(CFG.MODEL, features_only=True, pretrained=True)\n",
    "\n",
    "o = m(torch.randn(2, 3, 1024, 640))\n",
    "for x in o:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.utils.checkpoint as cp\n",
    "from timm.models.layers import SelectAdaptivePool2d, create_act_layer, create_attn\n",
    "from timm.models.resnet import Bottleneck, create_aa\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        cardinality=1,\n",
    "        base_width=64,\n",
    "        reduce_first=1,\n",
    "        dilation=1,\n",
    "        first_dilation=None,\n",
    "        act_layer=nn.ReLU,\n",
    "        norm_layer=nn.BatchNorm2d,\n",
    "        attn_layer=None,\n",
    "        aa_layer=None,\n",
    "        drop_block=None,\n",
    "        drop_path=None,\n",
    "    ):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        width = int(math.floor(planes * (base_width / 64)) * cardinality)\n",
    "        first_planes = width // reduce_first\n",
    "        outplanes = planes * self.expansion\n",
    "        first_dilation = first_dilation or dilation\n",
    "        use_aa = aa_layer is not None and (stride == 2 or first_dilation != dilation)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, first_planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = norm_layer(first_planes)\n",
    "        self.act1 = create_act_layer(act_layer, inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            first_planes,\n",
    "            width,\n",
    "            kernel_size=3,\n",
    "            stride=1 if use_aa else stride,\n",
    "            padding=first_dilation,\n",
    "            dilation=first_dilation,\n",
    "            groups=cardinality,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n",
    "        self.act2 = create_act_layer(act_layer, inplace=True)\n",
    "        self.aa = create_aa(aa_layer, channels=width, stride=stride, enable=use_aa)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width, outplanes, kernel_size=1, bias=False)\n",
    "        self.bn3 = norm_layer(outplanes)\n",
    "\n",
    "        self.se = create_attn(attn_layer, outplanes)\n",
    "\n",
    "        self.act3 = create_act_layer(act_layer, inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop_block(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.aa(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.se is not None:\n",
    "            x = self.se(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(shortcut)\n",
    "        x += shortcut\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetSelfDistill(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=CFG.MODEL,\n",
    "        pretrained=True,\n",
    "        act_layer=nn.Mish,\n",
    "        set_grad_checkpointing=False,\n",
    "        pos_weight=None,\n",
    "        drop_rate=0.2,\n",
    "        verbose=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            act_layer=act_layer,\n",
    "            features_only=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.drop_rate = drop_rate\n",
    "        self.feature_info = self.backbone.feature_info.channels()\n",
    "        self.set_grad_checkpointing = set_grad_checkpointing\n",
    "\n",
    "        self.neck = SelectAdaptivePool2d(pool_type=\"catavgmax\", flatten=True)\n",
    "        self.dropout = nn.Dropout(self.drop_rate)\n",
    "        self.cls_head = nn.Linear(self.feature_info[-1] * self.neck.feat_mult(), 1, bias=True)\n",
    "\n",
    "        self.bottleneck1 = Bottleneck(\n",
    "            self.feature_info[-3],\n",
    "            math.ceil(self.feature_info[-3] / 4),\n",
    "            act_layer=act_layer if act_layer is not None else nn.SiLU,\n",
    "        )\n",
    "        self.global_pool1 = SelectAdaptivePool2d(pool_type=\"avg\", flatten=True)\n",
    "        self.fc1 = nn.Linear(self.feature_info[-3], 1)\n",
    "\n",
    "        self.bottleneck2 = Bottleneck(\n",
    "            self.feature_info[-2],\n",
    "            math.ceil(self.feature_info[-2] / 4),\n",
    "            act_layer=act_layer if act_layer is not None else nn.SiLU,\n",
    "        )\n",
    "        self.global_pool2 = SelectAdaptivePool2d(pool_type=\"avg\", flatten=True)\n",
    "        self.fc2 = nn.Linear(self.feature_info[-2], 1)\n",
    "\n",
    "        self.cls_head.weight.data.normal_(0, 0.01)\n",
    "        self.cls_head.bias.data.zero_()\n",
    "\n",
    "        self.pos_weight = torch.FloatTensor(pos_weight) if pos_weight is not None else None\n",
    "        self.cls_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "\n",
    "        if verbose:\n",
    "            print(self)\n",
    "\n",
    "    def forward(self, img, return_distill_logits=True):\n",
    "        if self.set_grad_checkpointing:\n",
    "            feat_maps = cp.checkpoint(self.backbone, img)\n",
    "        else:\n",
    "            feat_maps = self.backbone(img)\n",
    "\n",
    "        feat_map_1, feat_map_2, feat_map_fin = feat_maps[-3], feat_maps[-2], feat_maps[-1]\n",
    "\n",
    "        x = self.neck(feat_map_fin)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cls_head(x).reshape(-1)\n",
    "\n",
    "        if return_distill_logits:\n",
    "            feat_map_1 = self.bottleneck1(feat_map_1)\n",
    "            feat_map_1 = self.global_pool1(feat_map_1)\n",
    "            feat_map_1 = self.dropout(feat_map_1)\n",
    "            distill_logit_1 = self.fc1(feat_map_1).reshape(-1)\n",
    "    \n",
    "            feat_map_2 = self.bottleneck2(feat_map_2)\n",
    "            feat_map_2 = self.global_pool2(feat_map_2)\n",
    "            feat_map_2 = self.dropout(feat_map_2)\n",
    "            distill_logit_2 = self.fc2(feat_map_2).reshape(-1)\n",
    "\n",
    "            return x, distill_logit_1, distill_logit_2,feat_map_1, feat_map_2, feat_map_fin\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41add6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:51.748231Z",
     "iopub.status.busy": "2022-10-21T04:46:51.747517Z",
     "iopub.status.idle": "2022-10-21T04:46:51.959456Z",
     "shell.execute_reply": "2022-10-21T04:46:51.958430Z"
    },
    "papermill": {
     "duration": 0.227045,
     "end_time": "2022-10-21T04:46:51.962222",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.735177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(RsnaDataset(train, augs=TRAIN_AUG, mode='train'),\n",
    "                          batch_size=2,\n",
    "                          shuffle=True,\n",
    "                          num_workers=8,\n",
    "                          drop_last=True,\n",
    "                        worker_init_fn=worker_init_fn)\n",
    "\n",
    "image,cancer = next(iter(dl))\n",
    "# a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3e892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:51.987500Z",
     "iopub.status.busy": "2022-10-21T04:46:51.987165Z",
     "iopub.status.idle": "2022-10-21T04:46:58.618975Z",
     "shell.execute_reply": "2022-10-21T04:46:58.616805Z"
    },
    "papermill": {
     "duration": 6.647161,
     "end_time": "2022-10-21T04:46:58.621465",
     "exception": false,
     "start_time": "2022-10-21T04:46:51.974304",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m = get_rsna_classification_model(CFG.MODEL)\n",
    "# m = NetSelfDistill()\n",
    "# x, distill_logit_1, distill_logit_2,feat_map_1, feat_map_2, feat_map_fin = m(image)\n",
    "# cancer1 = m(image)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bc3e1",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalanceSampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(self, dataset, ratio = 3):\n",
    "        self.r = ratio-1\n",
    "        self.dataset = dataset\n",
    "        self.pos_index = np.where(dataset.df.cancer>0)[0]\n",
    "        self.neg_index = np.where(dataset.df.cancer==0)[0]\n",
    "\n",
    "        self.length = self.r * int(np.floor(len(self.neg_index)/self.r)) \n",
    "        self.ds_len =  self.length + (self.length // self.r) \n",
    "\n",
    "    def __iter__(self):\n",
    "        pos_index = self.pos_index.copy()\n",
    "        neg_index = self.neg_index.copy()\n",
    "        np.random.shuffle(pos_index)\n",
    "        np.random.shuffle(neg_index)\n",
    "\n",
    "        neg_index = neg_index[:self.length].reshape(-1,self.r)\n",
    "        #pos_index = np.random.choice(pos_index, self.length//self.r).reshape(-1,1)\n",
    "        pos_index = np.tile(pos_index, (len(neg_index) // len(pos_index)) + 1)[:len(neg_index)].reshape(-1,1)\n",
    "\n",
    "        index = np.concatenate([pos_index,neg_index],-1).reshape(-1)\n",
    "        return iter(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_imbalance_sampler(labels):\n",
    "    class_count = torch.bincount(labels.squeeze())\n",
    "    class_weighting = 1. / class_count\n",
    "    sample_weights = class_weighting[labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(labels))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc552f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExhaustiveWeightedRandomSampler(WeightedRandomSampler):\n",
    "    \"\"\"ExhaustiveWeightedRandomSampler behaves pretty much the same as WeightedRandomSampler\n",
    "    except that it receives an extra parameter, exaustive_weight, which is the weight of the\n",
    "    elements that should be sampled exhaustively over multiple iterations.\n",
    "\n",
    "    This is useful when the dataset is very big and also very imbalanced, like the negative\n",
    "    sample is way more than positive samples, we want to over sample positive ones, but also\n",
    "    iterate over all the negative samples as much as we can.\n",
    "\n",
    "    Args:\n",
    "        weights (sequence): a sequence of weights, not necessary summing up to one\n",
    "        num_samples (int): number of samples to draw\n",
    "        exaustive_weight (int): which weight of samples should be sampled exhaustively,\n",
    "            normally this is the one that should not been over sampled, like the lowest\n",
    "            weight of samples in the dataset.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: Sequence[float],\n",
    "        num_samples: CFG.BS,\n",
    "        exaustive_weight=1,\n",
    "        generator=None,\n",
    "    ) -> None:\n",
    "        super().__init__(weights, num_samples, True, generator)\n",
    "        self.all_indices = torch.tensor(list(range(num_samples)))\n",
    "        self.exaustive_weight = exaustive_weight\n",
    "        self.weights_mapping = torch.tensor(weights) == self.exaustive_weight\n",
    "        self.remaining_indices = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    def get_remaining_indices(self) -> torch.Tensor:\n",
    "        remaining_indices = self.weights_mapping.nonzero().squeeze()\n",
    "        return remaining_indices[torch.randperm(len(remaining_indices))]\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        rand_tensor = torch.multinomial(\n",
    "            self.weights, self.num_samples, self.replacement, generator=self.generator\n",
    "        )\n",
    "        exaustive_indices = rand_tensor[\n",
    "            self.weights_mapping[rand_tensor].nonzero().squeeze()\n",
    "        ]\n",
    "        while len(exaustive_indices) > len(self.remaining_indices):\n",
    "            self.remaining_indices = torch.cat(\n",
    "                [self.remaining_indices, self.get_remaining_indices()]\n",
    "            )\n",
    "        yield_indexes, self.remaining_indices = (\n",
    "            self.remaining_indices[: len(exaustive_indices)],\n",
    "            self.remaining_indices[len(exaustive_indices) :],\n",
    "        )\n",
    "        rand_tensor[\n",
    "            (rand_tensor[..., None] == exaustive_indices).any(-1).nonzero().squeeze()\n",
    "        ] = yield_indexes\n",
    "        yield from iter(rand_tensor.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eabf0c",
   "metadata": {},
   "source": [
    "### Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185562dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Customn losss fnc\n",
    "\n",
    "### Customn losss fnc\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = F.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        return torch.mean(focal_loss)\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module): \n",
    "    def __init__(self, classes=1, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "  \n",
    "\n",
    "class CustomAuxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.8):\n",
    "        super(CustomAuxLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "\n",
    "    def forward(self, x, distill_logit_1, distill_logit_2,feat_map_1, feat_map_2, feat_map_fin, gt_label, batch_ix):\n",
    "#         print(feat_map_1.shape, feat_map_2.shape, feat_map_fin.shape)\n",
    "        \n",
    "        x, distill_logit_1, distill_logit_2 = x.view(-1, 1), distill_logit_1.view(-1, 1), distill_logit_2.view(-1, 1)\n",
    "        gt_label = gt_label.view(-1, 1)\n",
    "        \n",
    "        x_loss = F.binary_cross_entropy_with_logits(x, gt_label)\n",
    "\n",
    "        loss_1 = F.binary_cross_entropy_with_logits(distill_logit_1, gt_label)\n",
    "        loss_2 = F.binary_cross_entropy_with_logits(distill_logit_2, gt_label)\n",
    "\n",
    "        loss = x_loss + loss_1 + loss_2\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff2ad",
   "metadata": {
    "papermill": {
     "duration": 0.013027,
     "end_time": "2022-10-21T04:46:58.648053",
     "exception": false,
     "start_time": "2022-10-21T04:46:58.635026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train & Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597af09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T04:46:58.675824Z",
     "iopub.status.busy": "2022-10-21T04:46:58.674778Z",
     "iopub.status.idle": "2022-10-21T04:46:58.683272Z",
     "shell.execute_reply": "2022-10-21T04:46:58.682071Z"
    },
    "papermill": {
     "duration": 0.025267,
     "end_time": "2022-10-21T04:46:58.685455",
     "exception": false,
     "start_time": "2022-10-21T04:46:58.660188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pfbeta(labels, preds, beta=1,clip=True):\n",
    "    if clip:\n",
    "        preds = preds.clip(0, 1)\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "def pfbeta_thresh(labels, preds, beta=1):\n",
    "    preds = preds>0.1\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "def optimal_f1(labels, predictions):\n",
    "    labels = labels.cpu().numpy()\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    thres = np.linspace(0, 1, 100)\n",
    "    f1s = [pfbeta(labels, predictions > thr,clip=False) for thr in thres]\n",
    "    idx = np.argmax(f1s)\n",
    "    return f1s[idx], thres[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c498a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: Iterable,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler._LRScheduler = None,\n",
    "    mixup_fn: Callable = None,\n",
    "    grad_scaler: torch.cuda.amp.GradScaler = None,\n",
    "    mbar: master_bar = None,\n",
    "):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses_m = utils.AverageMeter()\n",
    "\n",
    "    pbar = progress_bar(loader, parent=mbar, leave=False)\n",
    "    pbar.update(0)\n",
    "\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        input, target  = input.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            x, distill_logit_1, distill_logit_2,feat_map_1, feat_map_2, feat_map_fin = model(input)\n",
    "            loss = loss_fn(x, distill_logit_1, distill_logit_2,feat_map_1, feat_map_2, feat_map_fin,target,batch_idx)\n",
    "        losses_m.update(loss.item(), input.size(0))\n",
    "\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        grad_scaler.step(optimizer)\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_norm)\n",
    "\n",
    "        grad_scaler.update()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        pbar.update(batch_idx + 1)\n",
    "        pbar.comment = f\"{losses_m.avg:.4f}\"\n",
    "\n",
    "    pbar.on_iter_end()\n",
    "    return OrderedDict([(\"loss\", losses_m.avg)])\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def validate(model: nn.Module, loader: Iterable, loss_fn: Callable, mbar: master_bar):\n",
    "    model.eval()\n",
    "\n",
    "    metric_m = utils.AverageMeter()\n",
    "    metric_m_thresh = utils.AverageMeter()\n",
    "    auc_m = utils.AverageMeter()\n",
    "    losses_m = utils.AverageMeter()\n",
    "\n",
    "    pbar = progress_bar(loader, parent=mbar, leave=False)\n",
    "    pbar.update(0)\n",
    "    \n",
    "    out = []\n",
    "    real = []\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        x, _, _,_, _, _ = model(input)\n",
    "\n",
    "        loss = loss_fn(x, target).item()\n",
    "        losses_m.update(loss, input.size(0))\n",
    "    \n",
    "        output = F.sigmoid(x)\n",
    "        metric = pfbeta(target,output).item()\n",
    "        metric_thresh,_ = optimal_f1(target, output)\n",
    "        metric_m.update(metric, input.size(0))\n",
    "        metric_m_thresh.update(metric_thresh.item(), input.size(0))\n",
    "        pbar.update(batch_idx + 1)\n",
    "        out.append(output)\n",
    "        real.append(target)\n",
    "    optf1, _ = optimal_f1(torch.cat(real), torch.cat(out))\n",
    "    auc = roc_auc_score(torch.cat(real).cpu().numpy(),torch.cat(out).cpu().numpy())\n",
    "    pbar.on_iter_end()\n",
    "    return OrderedDict([(\"loss\", losses_m.avg), (\"metric\", metric_m.avg),(\"metric_thresh\", optf1),('auc',auc)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee4a01",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60917654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(fold):\n",
    "    \n",
    "    with IPyExperimentsPytorch(exp_enable=False, cl_set_seed=42, cl_compact=True):\n",
    "        print()\n",
    "        print(\"*\" * 100)\n",
    "        print(f\"Training fold {fold}\")\n",
    "        print(\"*\" * 100)\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "      \n",
    "        dataset_train = RsnaDataset(train.query(\"fold!=@fold\").reset_index(drop=True), augs=TRAIN_AUG, mode=\"train\")\n",
    "        dataset_valid = RsnaDataset(train.query(\"fold==@fold\").reset_index(drop=True), augs=VALID_AUG, mode=\"valid\")\n",
    "\n",
    "        print(f\"TRAIN: {len(dataset_train)} | VALID: {len(dataset_valid)}\")\n",
    "\n",
    "        loader_train = torch.utils.data.DataLoader(dataset_train, \n",
    "                                                   CFG.BS, \n",
    "                                                   num_workers=8, \n",
    "                                                   drop_last=True,\n",
    "                                                  pin_memory=True,\n",
    "#                                                    sampler = ExhaustiveWeightedRandomSampler(train.query(\"fold!=@fold\").reset_index(drop=True).weight,num_samples=CFG.BS))\n",
    "                                                   shuffle=True)\n",
    "#                                                   sampler = class_imbalance_sampler(torch.tensor(train.query(\"fold!=@fold\").reset_index(drop=True)['cancer'].values)))\n",
    "#                                                    sampler=BalanceSampler(dataset_train))\n",
    "        loader_valid = torch.utils.data.DataLoader(dataset_valid, CFG.BS * 2, num_workers=8, shuffle=False)\n",
    "\n",
    "        model = NetSelfDistill()\n",
    "        model.cuda()\n",
    "        \n",
    "        optimizer = create_optimizer_v2(model, \"Adam\", lr=CFG.LR,weight_decay=CFG.WD)\n",
    "\n",
    "        num_train_steps = len(loader_train) * CFG.EP\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                           max_lr=CFG.LR,\n",
    "                                                           pct_start=0.1,\n",
    "                                                           total_steps=num_train_steps,\n",
    "                                                           verbose=False)\n",
    "\n",
    "        train_loss_fn = CustomAuxLoss()\n",
    "        valid_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        grad_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        print(f\"Scheduled epochs: {CFG.EP}\")\n",
    "\n",
    "        mbar = master_bar(list(range(CFG.EP)))\n",
    "        best_epoch, best_metric = 0, 100\n",
    "        metric_names = [\"epoch\", \"train_loss\", \"valid_loss\", \"metric\",\"metric_thresh\", \"auc\", \"time\"]\n",
    "        mbar.write([f\"{l:.6f}\" if isinstance(l, float) else str(l) for l in metric_names], table=True)\n",
    "#         alpha = 0.5\n",
    "        for epoch in range(CFG.EP):\n",
    "            \n",
    "#             train_loss_fn = CustomAuxLoss(alpha=alpha)\n",
    "                        \n",
    "            start_time = time.time()\n",
    "            mbar.update(epoch)\n",
    "            \n",
    "            train_metrics = train_one_epoch(\n",
    "                model, loader_train, train_loss_fn, optimizer,\n",
    "                lr_scheduler=lr_scheduler, mixup_fn=None, grad_scaler=grad_scaler, mbar=mbar)\n",
    "\n",
    "            valid_metrics = validate(model, loader_valid, valid_loss_fn, mbar=mbar)\n",
    "            \n",
    "            elapsed = format_time(time.time() - start_time)\n",
    "            epoch_log = [epoch,train_metrics[\"loss\"], valid_metrics[\"loss\"], valid_metrics[\"metric\"],\n",
    "                         valid_metrics[\"metric_thresh\"],valid_metrics[\"auc\"], elapsed]\n",
    "            mbar.write([f\"{l:.6f}\" if isinstance(l, float) else str(l) for l in epoch_log], table=True)\n",
    "\n",
    "#             if (valid_metrics[\"loss\"] < best_metric) or (1):\n",
    "            if 1:\n",
    "                best_epoch, best_metric = epoch, valid_metrics[\"loss\"]\n",
    "                path = Path(f'{MODEL_FOLDER}/fold_{fold}')\n",
    "                os.makedirs(path,exist_ok=True)\n",
    "                dirpath = path / (KERNEL_TYPE + f\"_Epoch_{epoch}_fold_{fold}.pth\")\n",
    "                torch.save(model.state_dict(), dirpath)\n",
    "            \n",
    "                \n",
    "        mbar.on_iter_end()\n",
    "        print(\"*** Best metric: {0} (epoch {1})\".format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e2c90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folds = [0,1,2,3]\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_idx in folds:\n",
    "        training_loop(fold_idx)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "del _\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6788d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_oof(fold):\n",
    "   \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dataset_valid = RsnaDataset(train.query(\"fold==@fold\").reset_index(drop=True), augs=VALID_AUG, mode=\"valid\")\n",
    "    ix =  train.query(\"fold==@fold\").index\n",
    "    print(f\"VALID: {len(dataset_valid)}\")\n",
    "\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, CFG.BS , num_workers=8, shuffle=False)\n",
    "    model = NetSelfDistill()\n",
    "    model.load_state_dict(torch.load(f'{MODEL_FOLDER}/fold_{fold}/{KERNEL_TYPE}_Epoch_{CFG.EP-1}_fold_{fold}.pth'))\n",
    "    model.cuda()  \n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    imageids = []\n",
    "\n",
    "    for input,label in tqdm(loader_valid, dynamic_ncols=True, desc=\"OOF Generation\"):\n",
    "        pred = []\n",
    "        with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "            input = input.cuda()\n",
    "            pred.append(F.sigmoid(model(input)[0]))\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        preds.append(torch.concat(pred).data.cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0),ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7efbb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof = np.zeros((len(train)))\n",
    "for k in tqdm(folds):\n",
    "    oof_fold,ix = gen_oof(k)\n",
    "    print(oof_fold.min(),oof_fold.max())\n",
    "    oof[ix] += oof_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c476ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_f1_numpy(oof,fold):\n",
    "    labels = train.loc[train['fold'].isin(fold)].reset_index(drop=True)['cancer'].values\n",
    "    oof = oof[train.loc[train['fold'].isin(fold)].index]\n",
    "    thres = np.linspace(0, 1, 100)\n",
    "    f1s = [pfbeta(labels, oof > thr,clip=False) for thr in thres]\n",
    "    idx = np.argmax(f1s)\n",
    "    return f1s[idx], thres[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr, thresh = optimal_f1_numpy(oof,folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c2681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scr,thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb57bf2",
   "metadata": {
    "id": "T0Z5tB5f8obu",
    "papermill": {
     "duration": 0.018419,
     "end_time": "2022-10-21T04:58:56.268413",
     "exception": false,
     "start_time": "2022-10-21T04:58:56.249994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fin "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 771.032802,
   "end_time": "2022-10-21T04:58:58.627815",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-21T04:46:07.595013",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
